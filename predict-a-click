## I tried to make things give better results by trying 4 different algoruthms to get the best possible prediction

##  Import necessary  libraies
#### We import needed packages for data preprocessing and models fitting

!pip install pandasql

import pandasql as ps


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns




from sklearn.preprocessing import LabelEncoder

from sklearn.model_selection import GridSearchCV, train_test_split
from xgboost import XGBRegressor

from sklearn.metrics import r2_score
from sklearn.metrics import mean_squared_error
from sklearn.cluster import KMeans


import pandas as pd
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import seaborn as sns
import matplotlib.pyplot as plt
from math import sqrt
%matplotlib inline

import math
from scipy import stats

import xgboost
import csv as csv
from xgboost import plot_importance
from matplotlib import pyplot
from sklearn.model_selection import cross_val_score,KFold
from sklearn.metrics import mean_absolute_error
import matplotlib.pyplot as plt
from scipy.stats import skew
from collections import OrderedDict

!pip install -U -q PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
# Authenticate and create the PyDrive client.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)
link='https://drive.google.com/open?id=1m2GdiYeJFulUjbOTiciYY3h2Osw5F1mU'


## Importing data : 
#### We import data , read it into pandas dataset



fluff, id = link.split('=')
print (id) # Verify that you have everything after '='

downloaded = drive.CreateFile({'id':id}) 
downloaded.GetContentFile('test_set.csv')  

test=pd.read_csv('test_set.csv')

!pip install -U -q PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
# Authenticate and create the PyDrive client.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)
link='https://drive.google.com/open?id=1k2lw5exkiOGFBc0JXmApd-MnutcK9KZG'






fluff, id = link.split('=')
print (id) # Verify that you have everything after '='

downloaded = drive.CreateFile({'id':id}) 
downloaded.GetContentFile('train_set.csv')  

train=pd.read_csv('train_set.csv')

### then we do some exploration on data , its characterestics , and needed cleaning and processing

#### First we demonstrate how data is stored

train.head()

test.head()

## We know that data manipulation process must be run the same way on train and test datasets , thus we  merge them during data manipulation steps , then we split them before trying prediction algorithms

train1=train.copy()

train1.info()

train1['train'] = 1

test['train'] = 0 

train1=pd.concat([train1 , test] , axis=0)

#### we notice that the column , Hotel_id and City_id are stored as floats despite they are identifiers so they should be stored as strings, we are to modify that later

### Then we check missing values appearence in data

for i in train1.columns :
  print(i + " contains " , np.isnan(train1[i]).sum() , "NaN value")

#### we notice a column like city_id includes 508 missing values , we will need later to wxclude them from the data , especiiallt that it is big enough to not be affected by these cases

####  then we will need to impute other columns , they are numeric , thus average is the most common option to impute them 

train1.loc[np.isnan(train1['city_id']) , ['avg_rank']].mean()

train1.loc[np.isnan(train1['city_id']) ==0 , ['avg_rank']].mean()

### about city_id , which is a categorical column , we will need to define a binary column for each option , but since data is big , and otions in city_id are a lot and a lot of them were only mentioned one time , we will need to specify columns for only cities that were mentioned more than a specific number

train1['city_id'].value_counts().head(10)

#### thus we specify a new column includes cities that were mentioned only more than 1000 times

f=list(train1['city_id'].value_counts()[(train1['city_id'].value_counts() > 1000)].index)


train1['city_id1'] = 0 

train1['city_id1'] = [x if x in f  else 0  for x in train1['city_id']]

train1['city_id1'].value_counts().head()

pd.options.display.float_format = "{:.2f}".format

#### then , let's describr the dataset : 

train1.describe()

#### we have another problem in the data , the column 'n_images' includes negative values , thus we need to inplace them.

#### Since it may indicate negative values , we replace it with the whole population mean

### but before that , we define a new dataset includes only cases with valid City_id

numeric_feats = ['content_score','n_images','distance_to_center' ,'stars','n_reviews','avg_rank','avg_price','avg_saving_percent']

train_new=train1.loc[np.isnan(train1['city_id'])==0 , :]



avg_n_images = train_new.loc[(train_new['n_images'] == -1)==0 ,: ]['n_images'].mean()

avg_n_images

train_new['n_images']=np.where(train_new['n_images'] ==-1 , avg_n_images , train_new['n_images'])

### then we demonstrate the distribution of each numeric columns

 for i in numeric_feats:
  print('distribution of ' , i)
  sns.distplot(train_new.loc[np.isnan(train_new[i]) == 0 ,:][i])
  plt.show()

### we notice that there are outliers in almost all columns , we replace them by bos plot whiskers as limits  

numeric_lower=[]
numeric_upper=[]
for i in numeric_feats:
    bp_i = plt.boxplot(train_new.loc[np.isnan(train[i]) == 0 ,:][i])
    lower_limit_i , upper_limit_i=[item.get_ydata()[1] for item in bp_i['whiskers']]
    numeric_lower.append(lower_limit_i)
    numeric_upper.append(upper_limit_i)
    

#### then we demonstrate upper and lower limits 

numeric_lower

numeric_upper

### then we replace outliers by these limits to make data more smooth

j=0
for i  in numeric_feats : 
    train_new[i]=np.where(train_new[i] < numeric_lower[j] , numeric_lower[j] , train_new[i])
    train_new[i]=np.where(train_new[i] > numeric_upper[j] , numeric_upper[j] , train_new[i])
    j+=1

#### then let's check data distributions :

 for i in numeric_feats:
  print('distribution of ' , i)
  sns.distplot(train_new.loc[np.isnan(train_new[i]) == 0 ,:][i])
  plt.show()

#### we notice that distributions became much more acceptable especially clear in average price , average rank and  n_reviews

#### based on the data pure of non logical values and outliers , we can take averages safely to replace outliers

for i in train_new.columns :
   print(i + " contains " , np.isnan(train_new[i]).sum() , "NaN value")

avg_avg_rating=train_new['avg_rating'].mean()

avg_n_images=train_new['n_images'].mean()

avg_distance_to_center=train_new['distance_to_center'].mean()

avg_n_reviews=train_new['n_reviews'].mean()

avg_avg_price=train_new['avg_price'].mean()

avg_stars=train_new['stars'].mean() 

avg_avg_saving_percent=train_new['avg_saving_percent'].mean()

train_new['avg_rating']=np.where(np.isnan(train_new['avg_rating']), avg_avg_rating , train_new['avg_rating'] )

train_new['n_images']=np.where(np.isnan(train_new['n_images']), avg_n_images , train_new['n_images'] )

train_new['distance_to_center']=np.where(np.isnan(train_new['distance_to_center']), avg_distance_to_center , train_new['distance_to_center'] )

train_new['n_reviews']=np.where(np.isnan(train_new['n_reviews']), avg_n_reviews , train_new['n_reviews'] )

train_new['avg_price']=np.where(np.isnan(train_new['avg_price']), avg_avg_price , train_new['avg_price'] )

train_new['stars']=np.where(np.isnan(train_new['stars']), avg_stars , train_new['stars'] )

train_new['avg_saving_percent']=np.where(np.isnan(train_new['avg_saving_percent']), avg_avg_saving_percent , train_new['avg_saving_percent'] )

for i in train_new.columns :
   print(i + " contains " , np.isnan(train_new[i]).sum() , "NaN value")

### then we modify type of columns , Hotel_id to be the index 

train_new=train_new.set_index('hotel_id')

train_new.head()

numeric_feats = ['content_score','n_images','distance_to_center' ,'stars','n_reviews','avg_rank','avg_price','avg_saving_percent']

#### also make city_id string (despite we will have to exclude it later)

train_new['city_id']=train_new['city_id'].astype('str')

train_new.info()

train_new.head()

### then define indicator variable for each city had frequency of 1000 or more in the data

train_final=pd.get_dummies(train_new , columns=['city_id1'])

## After steps of data manipulation , we again separate datasets , to train and test datasets before trying predictions algorithms to predict n_clicks

train_final=train_final.loc[train_final['train'] ==1 ,: ]

test_final = train_final.loc[train_final['train'] ==0 ,: ]

train_final1=train_final.copy()

#### then for better estimation we try log transformation to get rid of skewness in the predictors

train_final1[numeric_feats] = np.log1p(train_final1[numeric_feats])




 for i in numeric_feats:
  print('distribution of ' , i)
  sns.distplot(train_final1.loc[np.isnan(train_final1[i]) == 0 ,:][i])
  plt.show()



train_final1.head()

train_final1.info()

train_final1=train_final1.drop(columns=['city_id1_0.0' , 'city_id'])

train_final1.info()

train_final1.head()

### we notice that data is still skewed , since log transformation may need to lower score , especially if used log-log transformation , and also it did not succed in fixing skewness in predictors , we first use data as it is 

train_final.head()



x_train=train_final.drop(columns=['n_clicks' , 'city_id1_0.0' , 'city_id'])

y_train=train_final['n_clicks']

#### we divide training data into train - test sets , making train set 70% of data

# Organize our data for training
X = x_train
Y = y_train
X, X_Val, Y, Y_Val = train_test_split(X, Y , train_size =0.7 , shuffle=True)

### We try basicly the linear regression

from sklearn.linear_model import LinearRegression

reg = LinearRegression().fit(X, Y)
pred_0=pd.DataFrame(reg.predict(X_Val)).set_index(Y_Val.index)


pred_0=pd.DataFrame(reg.predict(X_Val)).set_index(Y_Val.index)

pred_0.columns=['pred']

 pred_0['pred']=np.where(pred_0['pred'] <0 , 0 , pred_0['pred'])

# Print the r2 score
print(r2_score(Y_Val, np.round(pred_0['pred'],0)))
# Print the mse score
print(mean_squared_error(Y_Val,np.round(pred_0['pred'],0)))

#### we notice that R2 is extremely low and MSE is large , let's try another variant

### we use XGB regressor as a winning solution in several kaggle competitions , since data is big , we try first to get the best n_estimators and learning_rate as per r2 and MSE (i.e to get the best score in estimating in train set then we can estimate n_clicks better in test set)

### so let's start our parameter tuning process

#### Here we begin with tuning n_estimators and learnnig_rate with other parameters as default , and when we get the best learning_rate and n_estimators with defaults we begin to tune max_depth and sub_sample , then tune min_child_weight and colsample_bytree 

#### we will do that 2 times one with data as it is and another taking minmax_transfromation for predictors



# Organize our data for training
X = x_train
Y = y_train
X, X_Val, Y, Y_Val = train_test_split(X, Y , train_size =0.7 , shuffle=True)



# A parameter grid for XGBoost
params = {'learning_rate': [ 0.01, 0.03 , 0.05 ,0.1] , 'n_estimators' :[100,  300 , 500 , 800 , 1000]}

# Initialize XGB and GridSearch
xgb = XGBRegressor(nthread=-1) 

grid1 = GridSearchCV(xgb, params)
grid1.fit(X, Y)

# Print the r2 score
print(r2_score(Y_Val, grid1.best_estimator_.predict(X_Val))) 
# Print the mse score
print(mean_squared_error(Y_Val, grid1.best_estimator_.predict(X_Val))) 

grid1.best_estimator_





#### then based on them we get the best max_depth and subsample portion that gives best estimates for n_clicks



# A parameter grid for XGBoost
params = {'learning_rate': [0.05] ,  'n_estimators':[500]  , 'max_depth' : [i for i in range(3,8)], 'subsample' : [i/10.0 for i in range(3,8)] }

# Initialize XGB and GridSearch
xgb = XGBRegressor(nthread=-1) 

grid2 = GridSearchCV(xgb, params)
grid2.fit(X, Y)

# Print the r2 score
print(r2_score(Y_Val, grid2.best_estimator_.predict(X_Val))) 
# Print the mse score
print(mean_squared_error(Y_Val, grid2.best_estimator_.predict(X_Val))) 

grid2.best_estimator_

model=XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0,
             importance_type='gain', learning_rate=0.03, max_delta_step=0,
             max_depth=4, min_child_weight=1, missing=None, n_estimators=300,
             n_jobs=1, nthread=-1, objective='reg:linear', random_state=0,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,
             silent=None, subsample=0.7, verbosity=1)

### and so on for min_child_weight and colsample_bytree



# A parameter grid for XGBoost
params = {'learning_rate': [0.05] ,  'n_estimators':[300]  , 'max_depth' : [5], 'subsample' : [0.7] , 
         'min_child_weight' :[0.5 , 1 , 1.5 , 2] , 'colsample_bytree' : [i / 10.0 for i in range(1,7)]
       }

# Initialize XGB and GridSearch
xgb = XGBRegressor(nthread=-1) 

grid3 = GridSearchCV(xgb, params)
grid3.fit(X, Y)

# Print the r2 score
print(r2_score(Y_Val, grid3.best_estimator_.predict(X_Val))) 
# Print the mse score
print(mean_squared_error(Y_Val, grid3.best_estimator_.predict(X_Val))) 

grid3.best_estimator_

model=XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=0.5, gamma=0,
             importance_type='gain', learning_rate=0.05, max_delta_step=0,
             max_depth=4, min_child_weight=1.5, missing=None, n_estimators=800,
             n_jobs=1, nthread=-1, objective='reg:linear', random_state=0,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,
             silent=None, subsample=0.7, verbosity=1)

model.fit(X, Y)



# Print the r2 score
print(r2_score(Y_Val, np.round((grid3.best_estimator_.predict(X_Val)),0)))
# Print the mse score
print(mean_squared_error(Y_Val, np.round((grid3.best_estimator_.predict(X_Val)),0)))


pred_1 = pd.DataFrame(grid3.best_estimator_.predict(X_Val)).set_index(Y_Val.index)



pred_1.columns=['pred']

 pred_1['pred']=np.where(pred_1['pred'] <0 , 0 , pred_1['pred'])

#### since predictions may include negative values , we replace them by zero , then calculate MSE and R2

# Print the r2 score
print(r2_score(Y_Val, np.round(pred_1['pred'],0)))
# Print the mse score
print(mean_squared_error(Y_Val, np.round(pred_1['pred'],0)))

## before , we tried fitting model with data pure without any transformation

### below we try min-max transformation on predictors 

numeric_feats = ['content_score','n_images','distance_to_center' ,'stars','n_reviews','avg_rank','avg_price','avg_saving_percent']

scaler = StandardScaler()

from sklearn.preprocessing import MinMaxScaler


cs = MinMaxScaler()


train_final2=train_final.copy()

train_final2=train_final2.drop(columns=['city_id1_0.0' , 'city_id'])

train_final2[numeric_feats] = cs.fit_transform(train_final2[numeric_feats])


x_train=train_final2.drop(columns=['n_clicks' ])

y_train=train_final2['n_clicks']

# Organize our data for training
X = x_train
Y = y_train
X, X_Val, Y, Y_Val = train_test_split(X, Y , train_size =0.7 , shuffle=True)

#### for better accuracy , we make dependent variable range between 0 and 1 as below :

max_clicks =y_train.max()
Y=Y/max_clicks
Y_Val =  Y_Val/max_clicks


### then we try first to get the best n_estimators and learning_rate as per r2 and MSE (i.e to get the best score in estimating in train set then we can estimate n_clicks better in test set)

### so let's start our parameter tuning process





# A parameter grid for XGBoost
params = {'learning_rate': [ 0.01, 0.03 , 0.05 ,0.1] , 'n_estimators' :[100,  300 , 500 , 800 , 1000]}

# Initialize XGB and GridSearch
xgb = XGBRegressor(nthread=-1) 

grid4 = GridSearchCV(xgb, params)
grid4.fit(X, Y)

# Print the r2 score
print(r2_score(Y_Val, grid4.best_estimator_.predict(X_Val))) 
# Print the mse score
print(mean_squared_error(Y_Val, grid4.best_estimator_.predict(X_Val))) 



grid4.best_estimator_

### and so on with the rest of parameters

# A parameter grid for XGBoost
params = {'learning_rate': [0.03] ,  'n_estimators':[300]  , 'max_depth' : [i for i in range(3,8)], 'subsample' : [i/10.0 for i in range(3,8)] }

# Initialize XGB and GridSearch
xgb = XGBRegressor(nthread=-1) 

grid5 = GridSearchCV(xgb, params)
grid5.fit(X, Y)

# Print the r2 score
print(r2_score(Y_Val, grid5.best_estimator_.predict(X_Val))) 
# Print the mse score
print(mean_squared_error(Y_Val, grid5.best_estimator_.predict(X_Val))) 

grid5.best_estimator_



# A parameter grid for XGBoost
params = {'learning_rate': [0.03] ,  'n_estimators':[300]  , 'max_depth' : [4], 'subsample' : [0.7] , 
         'min_child_weight' :[0.5 , 1 , 1.5 , 2] , 'colsample_bytree' : [i / 10.0 for i in range(1,7)]
       }

# Initialize XGB and GridSearch
xgb = XGBRegressor(nthread=-1) 

grid6 = GridSearchCV(xgb, params)
grid6.fit(X, Y)

# Print the r2 score
print(r2_score(Y_Val, grid6.best_estimator_.predict(X_Val))) 
# Print the mse score
print(mean_squared_error(Y_Val, grid6.best_estimator_.predict(X_Val))) 

grid6.best_estimator_

Y_Val = Y_Val * max_clicks
Y_pred = (grid6.best_estimator_.predict(X_Val)) * max_clicks

# Print the r2 score
print(r2_score(Y_Val, Y_pred)) 
# Print the mse score
print(mean_squared_error(Y_Val,Y_pred)) 



pred_2=pd.DataFrame(Y_pred ).set_index(Y_Val.index)

pred_2.columns=['pred']

pred_2 = pred_2['pred']=np.where(pred_2['pred'] <0 , 0 , pred_2['pred'])



# Print the r2 score
print(r2_score(Y_Val, pred_2)) 
# Print the mse score
print(mean_squared_error(Y_Val,pred_2)) 




### we third try deep learning in regression , making the same transformation of minmax , and make Y range between 0 , 1

# Organize our data for training
X = x_train
Y = y_train
X, X_Val, Y, Y_Val = train_test_split(X, Y , train_size =0.7 , shuffle=True)

max_clicks =y_train.max()
Y=Y/max_clicks
Y_Val =  Y_Val/max_clicks


X=np.matrix(X)
X_Val = np.matrix(X_Val)
Y=np.array(Y)
Y_Val=np.array(Y_Val)

!pip install keras

import keras
import numpy as np
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasRegressor

seed = 7
np.random.seed(seed)

# Model
model = Sequential()
model.add(Dense(200, input_dim=14, kernel_initializer='normal', activation='relu'))
model.add(Dense(100, kernel_initializer='normal', activation='relu'))
model.add(Dense(50, kernel_initializer='normal', activation='relu'))
model.add(Dense(25, kernel_initializer='normal', activation='relu'))
model.add(Dense(1, kernel_initializer='normal'))
# Compile model
model.compile(loss='mean_squared_error', optimizer=keras.optimizers.Adadelta())


model.fit(np.array(X), np.array(Y), epochs=100, batch_size=10)

pred_3 = model.predict(np.array(X_Val))


pred_3

pred_3=pd.DataFrame(pred*max_clicks).set_index(Y_original.index)

pred_3.columns=['pred']

pred_3.head()

pred['pred']=np.where(pred['pred'] <0 , 0 , pred['pred'])



Y_Val=pd.DataFrame(Y_Val*max_clicks)

# Print the r2 score
print(r2_score(Y_Val, pred)) 
# Print the mse score
print(mean_squared_error(Y_Val,pred)) 



1. Can you describe in your own words what the purpose of the evaluation
metric above is? What alternative metrics would make sense in this context?
it is the mean squared arror it measures the mean of errors (difference between predicted and actual values) we get the squared alue since , as known , errors may be negative or positive so if we take mean without squaring we may get lower value than actual , the measures other than that that make sence calculated with this formula
as clear it , reverse from MSE m it measures goodness of fit , or which percentage of variability in the dependent variable our model could explain.


2. We mention the click prediction as one component of our Exposure Algorithms. What other components would you include to determine what
advertiser or hotel to show to our users?

number of clickouts , total booking value , number of bookings , especially the last 2 are thr most important to the advertiser 

3. Which of the input variables have a high predictive power? What additional
variables would you include to reduce the error further?



4. In addition to the model you used to calculate your results what are alternative models could you use for the prediction problem? What are trade-offs
between the model you used and the alternatives?
